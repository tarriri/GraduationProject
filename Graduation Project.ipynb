{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Natural Language Processing Applied to Collected News Articles with NLTK\n",
    "\n",
    "## Members\n",
    "\n",
    "1. First member: Tarık Kılıç / kilicsem@itu.edu.tr\n",
    "\n",
    "## Description of the project\n",
    "\n",
    "This project contains applications of Natural Language Processing such as keyword extraction on collected news articles of Turkish media. News articles are collected from Web Archive and for time span of 2 months (December 2016-January 2017). Goal of the project is to extract most important keywords and concepts from this data source by using Natural Language Processing techniques.\n",
    "\n",
    "### The methods to be used\n",
    "\n",
    "While processing data, Latent Semantic Analysis(LSA) concepts will be implemented and be analyzed comparatively. To sum up, LSA is a mathematical technique to extract and determine relationship between terms and concepts in context. It is an analysis method of text documents which provides latent, hidden concepts of semantics in it. In this project, context or in another words document data is news articles. Frequently mentioned terms and techniques of LSA will be explained in detail.\n",
    "\n",
    "* Document: Every seperate text body represent documents. In that case, each news article will be mentioned as document. \n",
    "\n",
    "* Term-Document Matrix: It is a two-dimensional matrix representing frequency of each term in each document. Rows are for terms and columns are for documents.\n",
    "\n",
    "* Term Frequency(tf): For each term in a document, the number of occurances is called term frequency. This must be noted that term frequencies are identical between documents \"X is better than Y\" and \"Y is better than X\". There could be different type of term frequency weighting such as raw count, adjusted term frequency or augmented term frequency. Term frequency adjusted to document length will be the choice in that project for normalization purposes.(Not sure about that part, bcs augmented term frequency could be better since our news article lentghs varies)\n",
    "\n",
    "* Inverse Document Frequency(idf) : In terms of relevancy, terms as \"is\" is weighting equally as \"better\" in term frequency. But it shouldn't weight as much as \"better\" in semantic context. To solve this problem, inverse document frequency is defined for measuring whether term is rare or common across all documents. Idf is calculated as \\begin{equation*} idf(t,D) = log(\\frac{N}{1+\\left|\\big\\{ d \\in D : t \\in d \\big\\}\\right|}) \\end{equation*} where N is the total number of the documents in collection, and $\\left|\\big\\{ d \\in D : t \\in d \\big\\}\\right|$ is the number of documents where term t appears. It is easy to observe in any document collection that idf of a rare term is high and idf of a common term is low.\n",
    "\n",
    "* tf-idf : tf-idf weight is calculated by the product of tf and idf. Behaviour of tf-idf can be categorized as \n",
    "    * highest whenever a term occurs many time within a small number of documents.\n",
    "    * lower when a term occurs fewer times in a document or occurs in many documents.\n",
    "    * lowest when a term occurs many times in all documents.\n",
    "\n",
    "* Stop words : Stop words can be defined as a group words which are filtered before processing of text. There is no standart list of stop words to be filtered. It can vary in different semantic contexts.\n",
    "\n",
    "* Stemming: It is a process of reducing inflected or derived words to their root form.\n",
    "\n",
    "### The data\n",
    "\n",
    "Data set used in this project consists news articles from all topics across various newspapers. We select 10 newspapers from Turkish media according to circulation statistics. Our data set is really dependent to Web Archive. Methodology to obtain data set has 2 parts;\n",
    "\n",
    "1. Url Extraction of Specified Date: Every newspaper has a web site and in a relational database(MySQL), homepage URL of every newspaper is recorded. For a specified date in the past, using [client library of Memento Protocol](https://github.com/mementoweb/py-memento-client) for Python, [Memento Protocol](http://www.mementoweb.org/about/) is used in order to get past version of URL. If there is no past version of specified date of URL, [Wayback Availability API](https://archive.org/help/wayback_api.php) of Internet Archive is used in order to extract URL.  \n",
    "\n",
    "2. Content-News Article URL Scraping: After obtaining homepage of specified date, we need to access to news articles of that date. In order to achieve that, all anchor tags of homepage needs to be scraped. This may not get all news articles in that date, but a sufficient majority of articles can be collected in that way. \n",
    "\n",
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://web.archive.org/web/20161130190715/http://www.hurriyet.com.tr/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration Url Extraction of Specified Date\n",
    "from memento_client import MementoClient\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import datetime\n",
    "\n",
    "# Date and url initialization.\n",
    "memento = MementoClient()\n",
    "date = datetime.datetime.strptime(\"01-12-2016\", \"%d-%m-%Y\")\n",
    "url = \"http://www.hurriyet.com.tr/\"\n",
    "wayback_url = \"http://archive.org/wayback/available?url={}&timestamp={}\"\n",
    "\n",
    "try:\n",
    "    memento_info = memento.get_memento_info(url, date)\n",
    "    if memento_info.get(\"mementos\"):\n",
    "        mementos = memento_info.get(\"mementos\")\n",
    "        if mementos is not None and mementos.get(\"closest\").get(\"uri\") and mementos.get(\"closest\").get(\"uri\").__len__() > 0:\n",
    "            past_version = mementos.get(\"closest\").get(\"uri\")[0]\n",
    "    raise Exception()\n",
    "except Exception:\n",
    "    timestamp = date.strftime(\"%Y%m%d%H%M%S\")\n",
    "    wayback_get = requests.get(wayback_url.format(url, timestamp))\n",
    "    if wayback_get.status_code == 200:\n",
    "        result = wayback_get.json()[\"archived_snapshots\"]\n",
    "        if result is not None and result.get(\"closest\") and result[\"closest\"][\"available\"] is True:\n",
    "            past_version = result[\"closest\"][\"url\"]\n",
    "\n",
    "%store past_version\n",
    "past_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'116 URL extracted from homepage'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstration of Content-News Article URL Scraping\n",
    "\n",
    "#This part is necessary for importing our defined modules such as Scraper and Utility\n",
    "import sys\n",
    "path_of_modules = '/GraduationProject/NewsAnalysis/GraduationProject'\n",
    "if path_of_modules not in sys.path:\n",
    "    sys.path.append(path_of_modules)\n",
    "\n",
    "# getting back past version of homepage url\n",
    "%store -r past_version\n",
    "\n",
    "from Scraper import Scraper\n",
    "from Utility import Utility\n",
    "scraper = Scraper()\n",
    "utility = Utility()\n",
    "import NewsAnalysisDatabase as db\n",
    "content_list = set()\n",
    "\n",
    "\n",
    "r_get_homepage = requests.get(past_version)\n",
    "if r_get_homepage.status_code == 200:\n",
    "    platform = db.get_platform_by_object_id(1)\n",
    "    if platform is not None:\n",
    "        content_list.update(scraper.get_anchor_list_for_domain(r_get_homepage.content, utility.get_fixed_domain(past_version), platform.urldomain))\n",
    "    \n",
    "str(len(content_list)) + ' URL extracted from homepage'\n",
    "%store content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for uri_inf in uri_list:\n",
    "    # Gets news text for a given specific url: makes get request, removes unnecessary text from response.\n",
    "    if uri_inf.iscollected == 0:\n",
    "        try:\n",
    "            r_get = requests.get(uri_inf.urltext)\n",
    "        except:\n",
    "            continue\n",
    "        if r_get.status_code == 200:\n",
    "            platform = Provider.get_platform_by_object_id(uri_inf.platformid)\n",
    "            if platform is not None:\n",
    "                scraped_news = scraper.get_news_text(r_get.content, platform)\n",
    "                if scraped_news and scraped_news.strip():\n",
    "                    Provider.create_collected_news(scraped_news, uri_inf.urltext)\n",
    "                    Provider.mark_as_collected_url(uri_inf.objectid)\n",
    "                    print(\"article added\")\n",
    "                else:\n",
    "                    print(\"Nothing found on: \" + uri_inf.urltext)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
